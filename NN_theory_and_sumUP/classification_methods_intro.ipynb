{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a01704a",
   "metadata": {},
   "source": [
    "\n",
    "# Singular Value Decomposition (SVD)\n",
    "\n",
    "SVD is a matrix factorization technique useful in signal processing, statistics, and machine learning.\n",
    "\n",
    "Given a matrix \\( X \\in \\mathbb{R}^{m \\times n} \\), the SVD is given by:\n",
    "\n",
    "$$\n",
    "X = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( U \\in \\mathbb{R}^{m \\times m} \\) is an orthogonal matrix (left singular vectors)\n",
    "- \\( \\Sigma \\in \\mathbb{R}^{m \\times n} \\) is a diagonal matrix with non-negative singular values\n",
    "- \\( V \\in \\mathbb{R}^{n \\times n} \\) is an orthogonal matrix (right singular vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186ab0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Example matrix\n",
    "X = np.random.rand(5, 3)\n",
    "U, s, VT = np.linalg.svd(X, full_matrices=False)\n",
    "print(\"U shape:\", U.shape)\n",
    "print(\"s:\", s)\n",
    "print(\"VT shape:\", VT.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29d05a4",
   "metadata": {},
   "source": [
    "\n",
    "# From SVD to PCA\n",
    "\n",
    "PCA can be derived from the SVD of the centered data matrix. Let \\( X \\) be the data matrix, and \\( X_{\\text{mean}} \\) be the mean of the columns.\n",
    "\n",
    "We project the data into the principal components using:\n",
    "\n",
    "$$\n",
    "\\Phi = U^T (X - \\bar{X})\n",
    "$$\n",
    "\n",
    "To visualize:\n",
    "\n",
    "```python\n",
    "Phi = np.matmul(U.transpose(), X - X_mean[:, None])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c695e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_mean = X.mean(axis=1)\n",
    "Phi = np.matmul(U.T, X - X_mean[:, None])\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(Phi[0, :], Phi[1, :])\n",
    "ax.set_aspect('equal')\n",
    "plt.title(\"PCA Projection\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9b83d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Binary classification using projection on PC_1\n",
    "threshold = 0.0\n",
    "digits = [0, 1]\n",
    "labels_test = np.random.choice(digits, X.shape[1])\n",
    "A_test = X\n",
    "A_mean = X.mean(axis=1)\n",
    "\n",
    "PC_1 = np.matmul(U[:, 0].T, (A_test - A_mean[:, None]))\n",
    "\n",
    "labels_predicted = np.empty(labels_test.shape).astype(int)\n",
    "labels_predicted[PC_1 > threshold] = digits[0]\n",
    "labels_predicted[PC_1 <= threshold] = digits[1]\n",
    "\n",
    "true_0 = np.sum(np.logical_and(labels_test == digits[0], labels_predicted == digits[0]))\n",
    "false_0 = np.sum(np.logical_and(labels_test == digits[1], labels_predicted == digits[0]))\n",
    "true_1 = np.sum(np.logical_and(labels_test == digits[1], labels_predicted == digits[1]))\n",
    "false_1 = np.sum(np.logical_and(labels_test == digits[0], labels_predicted == digits[1]))\n",
    "\n",
    "print(f\"true  {digits[0]}: {true_0}\")\n",
    "print(f\"false {digits[0]}: {false_0}\")\n",
    "print(f\"true  {digits[1]}: {true_1}\")\n",
    "print(f\"false {digits[1]}: {false_1}\")\n",
    "accuracy = (true_0 + true_1) / (true_0 + true_1 + false_0 + false_1)\n",
    "print(f\"accuracy = {accuracy * 100:.2f} %\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ccec93",
   "metadata": {},
   "source": [
    "\n",
    "# Least Squares Regression\n",
    "\n",
    "We consider the linear model:\n",
    "\n",
    "$$\n",
    "y = mx + q\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( m = 2 \\), \\( q = 3 \\)\n",
    "- \\( x_i \\sim \\mathcal{N}(0, 1) \\)\n",
    "- \\( \\epsilon_i \\sim \\mathcal{N}(0, 2^2) \\)\n",
    "- \\( \\tilde{y}_i = y_i + \\epsilon_i \\)\n",
    "\n",
    "We want to recover \\( m \\) and \\( q \\) using the least squares method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758e18b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate data\n",
    "N = 100\n",
    "m_true = 2\n",
    "q_true = 3\n",
    "sigma = 2\n",
    "\n",
    "x = np.random.randn(N)\n",
    "y = m_true * x + q_true\n",
    "noise = np.random.normal(0, sigma, size=N)\n",
    "y_tilde = y + noise\n",
    "\n",
    "# Plot\n",
    "plt.scatter(x, y_tilde, label='Noisy Data', alpha=0.7)\n",
    "plt.plot(np.sort(x), m_true * np.sort(x) + q_true, color='red', label='True Line')\n",
    "plt.legend()\n",
    "plt.title(\"Linear Model with Noise\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8333bb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Least Squares using Moore-Penrose Pseudoinverse\n",
    "X_design = np.vstack((x, np.ones_like(x))).T\n",
    "theta = np.linalg.pinv(X_design) @ y_tilde\n",
    "\n",
    "# Predicted line\n",
    "x_pred = np.linspace(x.min(), x.max(), 100)\n",
    "y_pred = theta[0] * x_pred + theta[1]\n",
    "\n",
    "# Plot\n",
    "plt.scatter(x, y_tilde, label='Noisy Data', alpha=0.7)\n",
    "plt.plot(x_pred, y_pred, label='Least Squares Fit', color='green')\n",
    "plt.plot(np.sort(x), m_true * np.sort(x) + q_true, color='red', label='True Line')\n",
    "plt.legend()\n",
    "plt.title(\"Least Squares Fit\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3011694",
   "metadata": {},
   "source": [
    "\n",
    "# Nonlinear Regression and Kernel Methods\n",
    "\n",
    "We now consider the function:\n",
    "\n",
    "$$\n",
    "f(x) = \\tanh(2x - 1)\n",
    "$$\n",
    "\n",
    "We'll perform standard regression and then explore different kernel methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d101ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate data\n",
    "N = 100\n",
    "x = np.random.randn(N)\n",
    "y = np.tanh(2 * x - 1)\n",
    "noise = np.random.normal(0, 0.1, size=N)\n",
    "y_tilde = y + noise\n",
    "\n",
    "# Test data\n",
    "x_test = np.linspace(-3, 3, 1000)\n",
    "y_test_true = np.tanh(2 * x_test - 1)\n",
    "\n",
    "# Plot\n",
    "plt.scatter(x, y_tilde, label='Noisy Train Data', alpha=0.5)\n",
    "plt.plot(x_test, y_test_true, label='True Function', color='black')\n",
    "plt.title(\"Nonlinear Target Function\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90dc883",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Kernel Regression: Polynomial and Gaussian\n",
    "def kernel_matrix(X1, X2, kernel_fn):\n",
    "    return np.array([[kernel_fn(xi, xj) for xj in X2] for xi in X1])\n",
    "\n",
    "def poly_kernel(q):\n",
    "    return lambda x, y: (x * y + 1)**q\n",
    "\n",
    "def gaussian_kernel(sigma):\n",
    "    return lambda x, y: np.exp(-(x - y)**2 / (2 * sigma**2))\n",
    "\n",
    "def kernel_regression(x_train, y_train, x_test, kernel_fn):\n",
    "    K = kernel_matrix(x_train, x_train, kernel_fn)\n",
    "    alpha = np.linalg.solve(K, y_train)\n",
    "    K_test = kernel_matrix(x_test, x_train, kernel_fn)\n",
    "    return K_test @ alpha\n",
    "\n",
    "# Apply kernels\n",
    "k_poly = poly_kernel(q=2)\n",
    "k_gauss = gaussian_kernel(sigma=0.5)\n",
    "\n",
    "y_pred_poly = kernel_regression(x, y_tilde, x_test, k_poly)\n",
    "y_pred_gauss = kernel_regression(x, y_tilde, x_test, k_gauss)\n",
    "\n",
    "# Plot results\n",
    "plt.plot(x_test, y_test_true, label='True Function', color='black')\n",
    "plt.plot(x_test, y_pred_poly, label='Poly Kernel (q=2)', color='blue')\n",
    "plt.plot(x_test, y_pred_gauss, label='Gaussian Kernel (Ïƒ=0.5)', color='orange')\n",
    "plt.scatter(x, y_tilde, label='Train Data', alpha=0.3)\n",
    "plt.legend()\n",
    "plt.title(\"Kernel Regression\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edea4b53",
   "metadata": {},
   "source": [
    "\n",
    "# Support Vector Regression (SVR)\n",
    "\n",
    "SVR aims to find a function that deviates from the actual data points by a value no greater than \\( \\epsilon \\), while also being as flat as possible.\n",
    "\n",
    "The SVR loss function is defined as:\n",
    "\n",
    "$$\n",
    "L(w) = \\lambda \\|w\\|^2 + \\frac{1}{n} \\sum_{i=1}^{n} \\max(0, |f(x_i) - y_i| - \\epsilon)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( \\lambda \\): regularization parameter\n",
    "- \\( \\epsilon \\): tolerance margin\n",
    "- \\( f(x_i) \\): predicted output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c88be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "class SVR:\n",
    "    def __init__(self, lmbda=1.0, epsilon=0.1):\n",
    "        self.lmbda = lmbda\n",
    "        self.epsilon = epsilon\n",
    "        self.w = None\n",
    "\n",
    "    def loss(self, params, X, y):\n",
    "        pred = jnp.dot(X, params)\n",
    "        loss_val = jnp.maximum(0, jnp.abs(pred - y) - self.epsilon)\n",
    "        reg_term = self.lmbda * jnp.sum(params**2)\n",
    "        return reg_term + jnp.mean(loss_val)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        n_features = X.shape[1]\n",
    "        self.w = jnp.zeros(n_features)\n",
    "\n",
    "        opt_res = jax.scipy.optimize.minimize(self.loss, self.w, method=\"BFGS\", args=(X, y))\n",
    "        self.w = opt_res.x\n",
    "\n",
    "    def predict(self, X):\n",
    "        return jnp.dot(X, self.w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26ce8ae",
   "metadata": {},
   "source": [
    "\n",
    "# Support Vector Machine (SVM)\n",
    "\n",
    "SVM is a supervised learning model used for classification. It tries to find a hyperplane that separates the classes with the maximum margin.\n",
    "\n",
    "### Hinge Loss Function\n",
    "\n",
    "$$\n",
    "L(w) = \\lambda \\|w\\|^2 + \\frac{1}{n} \\sum_{i=1}^{n} \\max(0, 1 - y_i (w^T x_i + b))\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( \\lambda \\): regularization parameter\n",
    "- \\( y_i \\in \\{-1, +1\\} \\)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d4fb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SVM:\n",
    "    def __init__(self, lmbda=1.0):\n",
    "        self.lmbda = lmbda\n",
    "        self.w = None\n",
    "\n",
    "    def loss(self, params, X, y):\n",
    "        decision = jnp.dot(X, params[:-1]) + params[-1]\n",
    "        loss_val = jnp.maximum(0, 1 - y * decision)\n",
    "        reg_term = self.lmbda * jnp.sum(params[:-1] ** 2)\n",
    "        return reg_term + jnp.mean(loss_val)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        _, n_features = X.shape\n",
    "        self.w = jnp.zeros(n_features + 1)\n",
    "\n",
    "        opt_res = jax.scipy.optimize.minimize(self.loss, self.w, method=\"BFGS\", args=(X, y))\n",
    "        self.w = opt_res.x\n",
    "\n",
    "    def predict(self, X):\n",
    "        decision = jnp.dot(X, self.w[:-1]) + self.w[-1]\n",
    "        return jnp.sign(decision)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
